# -*- coding: utf-8 -*-
"""search_backend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iB5r9UKBQsbTvV9PRrp9fbFRpqXWCIuN
"""

!pip install -q pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
!pip install -q graphframes
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'
spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'
!wget -N -P $spark_jars $graphframes_jar

import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from graphframes import *
from pyspark.sql import SparkSession

from pyspark.sql import SparkSession
from pyspark import SparkConf
from pathlib import Path

# Set the path to the gcs-connector JAR file
gcs_connector_jar = "path/to/gcs-connector-hadoop3-2.2.2.jar"

# Initializing Spark context
# Create a Spark context and session
conf = SparkConf().set("spark.ui.port", "4050")
conf.set("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
conf.set("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
conf.set("spark.jars", gcs_connector_jar)  # Add the gcs-connector JAR to the Spark configuration
sc = pyspark.SparkContext(conf=conf)
sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))
spark = SparkSession.builder.getOrCreate()

#imports
import re
import numpy as np
import pickle
import nltk
import builtins
from nltk.corpus import stopwords
import json
from contextlib import closing
from nltk.stem import *
from collections import defaultdict, Counter
from inverted_index_gcp import *
import math
from google.cloud import storage
import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage
import builtins
import math
from nltk import ngrams


stemmer = PorterStemmer()

import hashlib
def _hash(s):
    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()

nltk.download('stopwords')



bucket = '315537936'
full_path = f"gs://{bucket}/"
client = storage.Client()
#bucket = client.get_bucket("315537936")

# Authenticate your user
# The authentication should be done with the email connected to your GCP account
from google.colab import auth
import signal

AUTH_TIMEOUT = 30000000 #It worked well on 30000 , all printings have done with this time

def handler(signum, frame):
  raise Exception("Authentication timeout!")

signal.signal(signal.SIGALRM, handler)
signal.alarm(AUTH_TIMEOUT)

try:
   auth.authenticate_user()
except:
   pass

from google.cloud import storage
import os
from google.colab import files

# Upload the JSON key file to Colab
uploaded = files.upload()

# Set the GCS credentials using the uploaded key file
key_file_name = 'task3mapreduce315537936-2e5ecd407dc5.json'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_file_name

# # if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir
# %cd -q /home/dataproc
# !ls inverted_index_gcp.py
# # adding our python module to the cluster
# sc.addFile("/home/dataproc/inverted_index_gcp.py")
# sys.path.insert(0,SparkFiles.getRootDirectory())
# from inverted_index_gcp import InvertedIndex

# bucket_name = '315537936'
# full_path = f"gs://{bucket_name}/"
# paths=[]

# client = storage.Client()
# blobs = client.list_blobs(bucket_name)
# for b in blobs:
#     if b.name != 'graphframes.sh':
#         paths.append(full_path+b.name)
# corpus  = spark.read.parquet(*paths)

# #spark
# # These will already be installed in the testing environment so disregard the
# # amount of time (~1 minute) it takes to install.
# !pip install -q pyspark
# !pip install -U -q PyDrive
# !apt install openjdk-8-jdk-headless -qq
# !pip install -q graphframes
# import os
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'
# spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'
# !wget -N -P $spark_jars $graphframes_jar

# nltk.download('stopwords')

# # %cd -q /home/dataproc
# # !ls inverted_index_gcp.py
# from inverted_index_gcp import InvertedIndex, MultiFileReader

# import math
# from google.cloud import storage
# bucket_name = '315537936'
# full_path = f"gs://{bucket_name}/"
# client = storage.Client()
# bucket = client.get_bucket("315537936")

# Uploading Indexes Without Stemming

client = storage.Client()

client = storage.Client.from_service_account_json('task3mapreduce315537936-2e5ecd407dc5.json')

bucket = client.get_bucket('315537936')

title_index = pickle.loads(bucket.get_blob('title/title_index.pkl').download_as_string())

#body_index = pickle.loads(bucket.get_blob('text/text_index.pkl').download_as_string())

# Uploading Indexes with Stemming Only

client = storage.Client()

client = storage.Client.from_service_account_json('task3mapreduce315537936-2e5ecd407dc5.json')

bucket = client.get_bucket('315537936')

title_stem = pickle.loads(bucket.get_blob('title_stem_only/title_stem_only_index.pkl').download_as_string())

body_stem = pickle.loads(bucket.get_blob('text_stem/text_stem_index.pkl').download_as_string())

# Uploading Indexes with N-gram Only

client = storage.Client()

client = storage.Client.from_service_account_json('task3mapreduce315537936-2e5ecd407dc5.json')

bucket = client.get_bucket('315537936')

title_gram = pickle.loads(bucket.get_blob('title2gram/title2gram_index.pkl').download_as_string())

#body_gram = pickle.loads(bucket.get_blob('text_2gram/text_2gram_index.pkl').download_as_string())

# Uploading Indexes with N-gram and stemming

client = storage.Client()

client = storage.Client.from_service_account_json('task3mapreduce315537936-2e5ecd407dc5.json')

bucket = client.get_bucket('315537936')

title_total = pickle.loads(bucket.get_blob('title_stem/title_stem_index.pkl').download_as_string())

body_total = pickle.loads(bucket.get_blob('text_stem2gram/text_stem2gram_index.pkl').download_as_string())

#Uploading useful dictionary pickle files

client = storage.Client()

client = storage.Client.from_service_account_json('task3mapreduce315537936-2e5ecd407dc5.json')

bucket = client.get_bucket('315537936')

#anchor_index = pickle.loads(bucket.get_blob('anchor/anchor_index.pkl').download_as_string())

pageviews = pickle.loads(bucket.get_blob('pageviews.pkl').download_as_string())

pagerank = pickle.loads(bucket.get_blob('pagerank.pkl').download_as_string())

idtitle = pickle.loads(bucket.get_blob('doc_title_dict.pkl').download_as_string())

TUPLE_SIZE = 6
stemmer = PorterStemmer()

english_stopwords = frozenset(stopwords.words('english'))
corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']
RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)

all_stopwords = english_stopwords.union(corpus_stopwords)

######################## Pre-Processing ###################################################################

def get_query_tokens(text, stemming=False):
    """
    Tokenizes the input query text and applies stemming (Optionally).
    Eventually, if we don't do stemming it will fit title_index & body_index.
    If we do stemming it will fit text with index_stemmed.

    Parameters:
        text (str): query to be tokenized.
        stemming (bool): If True, applies stemming. Default -  False.

    Returns:
        list: A list of tokens extracted from the input text.
    """
    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    if stemming:
        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]
    else:
        tokens = [term for term in tokens if term not in all_stopwords]
    return tokens

def get_tokens_ngrams(text,stemming=False):
    """
    Returns tokens of the input *query* with n-gram, suitable for main search.

    Parameters:
        text (list): The input - The query. Can be after stem or not.
        stemming (bool): If True, applies stemming. Default -  False.


    Returns:
        list: A list of tokens including n-gram generated from the input text.
    """
    ngrams_tokens = []
    # Re-check to not include stop-words
    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    if stemming:
        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]
    else:
        tokens = [term for term in tokens if term not in all_stopwords]
    bigram = list(ngrams(tokens, 2))
    ngrams_tokens += [' '.join(b) for b in bigram]

    return ngrams_tokens

def get_candidate_documents(query, index):
    """
    Returns relevant documents for the given query along with their term frequencies.
    Returns a set of all unique relevant documents their corresponding (token:{(doc_id,tf)...}).
    The tuple (doc_id,tf) is extracted from the index function 'read_a_posting_list'
    when recieving a specific token.

    Parameters:
        query_to_search (list): A list of tokens extracted from user's query.
        index: The inverted index containing relevant information.


    Returns:
        set: A set of unique relevant document IDs.
        dict: A dictionary of candidates where each key is a term and its value is a dictionary
              containing relevant document IDs and their corresponding term frequencies. Contains all words and relevant
              docs.
              Example structure:
              {
                  'term1': {'doc_id1': term_frequency1, 'doc_id2': term_frequency2, ...},
                  'term2': {'doc_id3': term_frequency3, 'doc_id4': term_frequency4, ...},
                  ...
              }
    """
    candidates_docs, candidates = {}, set()
    for w in np.unique(query):
        # cheaks if word in inverted index df which is {token:df}
        df = index.df
        if w in df:
            # Extracting (doc_id,tf) for specific token
            p = index.read_a_posting_list(".",w,"315537936")
            only_docs = [x[0] for x in p]
            candidates_docs.update({w: dict(p)})
            candidates.update(only_docs)
    return candidates, candidates_docs

def get_top_n_scored_docs(score_dict, N=500):
    """
    Sorts the best documents by their scores.
    The score will be defined later by linear combination of many scores.

    Parameters:
        sim_dict (dict): A dictionary {doc_id: scores}.
    Returns:
        list: The best N documents sorted in descending order by their scores.
              Default N is after a lot of trials. N=100 is the minimum possible.
    """
    return sorted([(doc_id, score) for doc_id, score in score_dict.items()], key=lambda x: x[1], reverse=True)[:N]

###################################################################################################################

###################  Search title and / or anchor index ################################################

def get_title_anchor_score(query, term_dict):
    """
    When we look at a certain index,
    the frequency of a certain word in relation to the size of the index is very low.
    Calculating a weighted score like tf-idf or CosSim will not give an accurate measure and can lead to bias calculation.
    Therefore, counting the total frequency that a certain word appears in a certain document is more reasonable.
    The more times a certain word appears in the title/ anchor of a document,
    the higher the score and the more relevant it will be for this specific query.
    This function calculates unique tokens from the query that appear in a document.

    Parameters:
        query (list): A list of tokens representing the query.
        term_dict (dict): A dictionary where keys are terms from the query and values are dictionaries
            containing relevant document IDs and their corresponding term frequencies.

    Returns:
        dict: A dictionary where keys are document IDs and values are scores based on how many unique words
              appear in each document -- > {doc_id:score}
    """
    dict_to_return = {}
    for w in term_dict.keys():
        for doc in term_dict[w].keys():
            dict_to_return[doc] = dict_to_return.get(doc, 0) + 1
    return dict_to_return

def search_for_title_anchor(query, index):
    """
    Returns the best documents for title and anchor.
    The documents are sorted by their score which is calculated above.

    Parameters:
        query (list): A list of tokens representing the query.
        index (dict): The inverted index containing information about terms and their corresponding documents.

    Returns:
        list: A list of tuples containing document IDs and their corresponding titles, sorted by score.
        dict: A dictionary where {doc_id:score}, helpful for the main search.
    """
    rel_docs, candidates_dict = get_candidate_documents(query, index)
    rr = get_title_anchor_score(query, candidates_dict)
    # N is after lots of expirements
    id_score = get_top_n_scored_docs(rr, N=100000000)
    res = [i[0] for i in id_score]
    return [(j, idtitle[j]) for j in res if j in idtitle], rr

########################  Search body index   ######################################################

def cos_sim_calculation(query_tokens, index, docs, candidates_dict):
    """
    Calculates TF-IDF and cosine similarity for body index.
    NOTICE - the calculation is ONLY for the relevant documents from the candidates.
    NOT FOR EVERY DOCUMENT!!!

    Parameters:
        query_tokens (list): A list of tokens representing the query.
        index: The inverted index containing significant information.
        candidate_dict (dict): A dictionary which: {term: {doc_id: tf}}.

    Returns:
        dict: A dictionary where {doc_id: score (cosSim)}.
    """
    # get size of query
    n = len(query_tokens)
    query_vector = np.ones(n)
    doc_score = defaultdict(int)

    for doc in docs:
        tf_ids_score = np.empty(n)
        for i, word in enumerate(query_tokens):
            # Only relevant docs of query
            if (word in index.df) and (doc in candidates_dict[word]):
                tf_term = (candidates_dict[word][doc] / index.document_length[doc])
                # CORPUS_SIZE = 6348910
                idf_term = math.log2(6348910 / index.df[word])
                tf_ids_score[i] = tf_term * idf_term
            else:
                tf_ids_score[i] = 0

        inner_product = np.dot(tf_ids_score, query_vector)
        size_q = np.linalg.norm(query_vector)
        # Cosine Similarity calculation for each candidate document
        doc_score[doc] = inner_product / (size_q * index.normalized_length[doc])

    return doc_score

def search_body(index, query):
    """
    Returns the best documents for body.
    The documents are sorted by their score which is calculated above.
    Parameters: query: list of tokens
                index: inverted index
    Returns: [(doc_id,title)...], {doc_id: cosine similarity score}
             The dictionary will be helpful for the main search.
    """
    rel_docs, candidates_dict = get_candidate_documents(query, index)
    cos_sim_dict = cos_sim_calculation(query, index, rel_docs, candidates_dict)
    # We will return first 100 because this is what requested
    top_n = get_top_n_scored_docs(cos_sim_dict, 100)
    res = [i[0] for i in top_n]
    return [(j, idtitle[j]) for j in res], cos_sim_dict

########################  Class for improving body index search #########################################


class BM25_index:
    """
    BM25 is a sub-linear transformation of TF-IDF calculation.
    It helps us avoid dominance by one term,
    creating an upper bound.
    It normalizes document length.
    BM25 has two empirical parameters. By tuning them, we can even improve it more.
    b - punishes long documents, k1 - allows for control over the saturation effect.
    We defined k1=1.5, b=0.75 after searching for the most effective values for tuning.
    """

    def __init__(self, index, k1=1.5, b=0.75):
        self.b = b
        self.k1 = k1
        self.index = index
        # CORPUS_SIZE
        self.N = len(index.document_length)
        # Average document length
        self.avg_doc_len = builtins.sum(index.document_length.values()) / self.N

    def calc_idf(self, list_of_tokens):
        idf = {}
        for term in list_of_tokens:
            if term in self.index.df:
                n_ti = self.index.df[term]
                # FORMULA
                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))
            else:
                pass
        return idf

    def _score(self, query, doc_id, candidate_dict):
        score = 0.0
        doc_len = self.index.document_length[doc_id]
        for term in query:
            if term in self.index.df:
                # (doc_id, tf) for each term
                term_frequencies = candidate_dict[term]
                if doc_id in term_frequencies:
                    freq = term_frequencies[doc_id]
                    # FORMULA
                    numerator = self.idf[term] * freq * (self.k1 + 1)
                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)
                    score += (numerator / denominator)
        return score

    def search(self, q, N=10000):
        # q is the extracted tokens list
        candidates, candidates_dict = get_candidate_documents(q, self.index)
        self.idf = self.calc_idf(q)
        temp_dict = {k: self._score(q, k, candidates_dict) for k in candidates}
        # highest_N_score is a list of tuples where [(docID, score)...]
        highest_n_score = get_top_n_scored_docs(temp_dict, N)
        return highest_n_score

#######################################################################################################

################################# Helper for main search ############################################
def find_minmax(scores_list):
    """
    Returns the minimum and maximum values in a given list.
    Helps us to scale the score from every score in the main search
    Parameter:
        lst (list of floats): A list of floats.
    Returns:
        tuple: A tuple containing the minimum and maximum numbers from the list.
    """
    if len(scores_list) == 0: return 0, 1
    min_v, max_v = np.min(scores_list), np.max(scores_list)
    if max_v == 0.0:
        max_v = 1
    return min_v, max_v

  #######################################################################################################

#####################      THE MAIN SEARCH   ##################################################\

def search(query, idx_title, idx_body, idx_title_ngram):
    """
    The main search function,
    This function calculates the best documents with the highest scores by:
    their body (stemmed)
    title
    title with n-gram
    page rank
    page views

    All the relevant docs are extracted after applying BM25 search on body index.
    Assuming it predicts the best candidates docs. N is also after lots of experiments.
    All the scores for relevant docs are first scaled by min max scaler.
    Then we give every score a ceratin weight in the final score calculation.
    The weights are after lots of expereiments to find the best combinaton.

    Paramters:
        query : The query as a string.
        idx_title_stem : Inverted index on title.
        idx_title_ngram : Inverted index on title with stemming.
        idx_body_stem : Inverted index on anchor.

    Returns:
        list: A list of tuples containing document IDs and their corresponding titles.
              The list includes the documents that received the highest scores based on various metrics
              while using weights- {doc_id:title}
    """

    # Tokens regular
    query_tokens = get_query_tokens(query)
    # Tokens with stem
    query_stem = get_query_tokens(query,True)
    # Tokens with n-gram
    #query_ngram = get_tokens_ngrams(query)
    # Tokens with n-gram and stem
    #query_ngram_stem = get_tokens_ngrams(query,True)

    # Search with bm25
    bm25 = BM25_index(idx_body)
    bm25_cand = bm25.search(query_tokens, 5000)


    cadndiate_scores = {}

    #INITIALIZE
    bm25_lst_scores = np.zeros(len(bm25_cand))
    top_titles_scores = np.zeros(len(bm25_cand))
    top_title_n_gram_scores = np.zeros(len(bm25_cand))
    page_rank_scores = np.zeros(len(bm25_cand))
    page_views_scores = np.zeros(len(bm25_cand))

    for i, c in enumerate(bm25_cand):
        bm25_lst_scores[i] = c[1]
        cadndiate_scores[c[0]] = [c[1], 0, 0, 0, 0]

    # search on title and anchor -> {doc_id:score}
    title_sim = search_for_title_anchor(query_stem, idx_title)[1]
    title_ngram_sim = search_for_title_anchor(query_tokens, idx_title_ngram)[1]

    top_title = get_top_n_scored_docs(title_sim, 5000)
    top_title_ngram = get_top_n_scored_docs(title_ngram_sim, 5000)


    for i, c in enumerate(top_title):
        if c[0] in cadndiate_scores:
            top_titles_scores[i] = c[1]
            cadndiate_scores[c[0]][1] = c[1]

    for i, c in enumerate(top_title_ngram):
        if c[0] in cadndiate_scores:
            top_title_n_gram_scores[i] = c[1]
            cadndiate_scores[c[0]][2] = c[1]

    for i, c in enumerate(cadndiate_scores):
        if c in pagerank:
            page_rank_scores[i] = pagerank[c]
            cadndiate_scores[c][3] = pagerank[c]
        if c in pageviews:
            page_views_scores[i] = pageviews[c]
            cadndiate_scores[c][4] = pageviews[c]

    # find min max values for normalize
    minimum_bm25, maximum_bm25 = find_minmax(bm25_lst_scores)
    minimum_title, maximum_title = find_minmax(top_titles_scores)
    minimum_ngram, maximum_ngram = find_minmax(top_title_n_gram_scores)
    minimum_prank, maximum_prank = find_minmax(page_rank_scores)
    minimum_pv, maximum_pv = find_minmax(page_views_scores)

    for k in cadndiate_scores:
        agg_score = 0
        # MIN MAX SCALER
        cadndiate_scores[k][0] = (cadndiate_scores[k][0] - minimum_bm25) / (maximum_bm25 - minimum_bm25) * 3
        cadndiate_scores[k][1] = (cadndiate_scores[k][1] - minimum_title) / (maximum_title - minimum_title) * 1
        cadndiate_scores[k][2] = (cadndiate_scores[k][2] - minimum_ngram) / (maximum_ngram - minimum_ngram) * 1
        cadndiate_scores[k][3] = (cadndiate_scores[k][3] - minimum_prank) / (maximum_prank - minimum_prank) * 1
        cadndiate_scores[k][4] = (cadndiate_scores[k][4] - minimum_pv) / (maximum_pv - minimum_pv) * 1

        # get aggregated score for each document
        for i in cadndiate_scores[k]:
          agg_score += i
          cadndiate_scores[k] = agg_score

    #REQUESTED TOP 50
    result = get_top_n_scored_docs(cadndiate_scores, 50)
    res = [i[0] for i in result]
    res = [(str(j), idtitle[j]) for j in res]

    return res

def final_search(query):
    """
    Calls to the final search.
    """
    return search(query, title_stem, body_stem, title_gram)

import json
import time
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score

with open('queries_train.json', 'rt') as f:
  queries = json.load(f)

def average_precision(true_list, predicted_list, k=40):
    true_set = frozenset(true_list)
    predicted_list = predicted_list[:k]
    precisions = []
    for i,doc_id in enumerate(predicted_list):
        if doc_id in true_set:
            prec = (len(precisions)+1) / (i+1)
            precisions.append(prec)
    if len(precisions) == 0:
        return 0.0
    return round(sum(precisions)/len(precisions),3)

def precision_at_k(true_list, predicted_list, k):
    true_set = frozenset(true_list)
    predicted_list = predicted_list[:k]
    if len(predicted_list) == 0:
        return 0.0
    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)
def recall_at_k(true_list, predicted_list, k):
    true_set = frozenset(true_list)
    predicted_list = predicted_list[:k]
    if len(true_set) < 1:
        return 1.0
    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)
def f1_at_k(true_list, predicted_list, k):
    p = precision_at_k(true_list, predicted_list, k)
    r = recall_at_k(true_list, predicted_list, k)
    if p == 0.0 or r == 0.0:
        return 0.0
    return round(2.0 / (1.0/p + 1.0/r), 3)
def results_quality(true_list, predicted_list):
    p5 = precision_at_k(true_list, predicted_list, 5)
    f1_30 = f1_at_k(true_list, predicted_list, 30)
    if p5 == 0.0 or f1_30 == 0.0:
        return 0.0
    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)

import time

results_dict = {}
precision_values=[]
recall_values = []
f1_values= []
quality_values = []
avg_precision_values = []
t_start = time.time()

for query_id, query_text in queries.items():
    # Assuming final_search returns a list of tuples (doc_id, title)
    predicted_results = final_search(query_id)

    # Extract doc_id values from predicted_results
    predicted_doc_ids = [doc_id for doc_id, title in predicted_results]

    # Calculate metrics
    precision = precision_at_k(query_text, predicted_doc_ids, 10)
    recall = recall_at_k(query_text, predicted_doc_ids, 10)
    f1 = f1_at_k(query_text, predicted_doc_ids, 10)
    quality = results_quality(query_text, predicted_doc_ids)
    avg_precision = average_precision(query_text, predicted_doc_ids, 40)

    # Store results in a dictionary
    query_results = {
        "precision_at_k": precision,
        "recall_at_k": recall,
        "f1_at_k": f1,
        "results_quality": quality,
        "average_precision": avg_precision
    }

    # Add to the main results dictionary
    results_dict[query_id] = query_results

# Print results for each query
for query_id, query_results in results_dict.items():
    print(f"Results for Query {query_id}:")
    print(f"Precision@10: {query_results['precision_at_k']}")
    print(f"Recall@10: {query_results['recall_at_k']}")
    print(f"F1@10: {query_results['f1_at_k']}")
    print(f"Results Quality: {query_results['results_quality']}")
    print(f"MAP@40: {query_results['average_precision']}")
    print("\n")

    # Append metric values for averaging
    precision_values.append(query_results['precision_at_k'])
    recall_values.append(query_results['recall_at_k'])
    f1_values.append(query_results['f1_at_k'])
    quality_values.append(query_results['results_quality'])
    avg_precision_values.append(query_results['average_precision'])

# Calculate and print averages
average_precision_at_k = round(sum(precision_values) / len(precision_values),3)
average_recall_at_k = round(sum(recall_values) / len(recall_values),3)
average_f1_at_k = round(sum(f1_values) / len(f1_values),3)
average_quality = round(sum(quality_values) / len(quality_values),3)
average_avg_precision = round(sum(avg_precision_values) / len(avg_precision_values),3)

print("\nAverages across all queries:")
print(f"Average Precision@10: {average_precision_at_k}")
print(f"Average Recall@10: {average_recall_at_k}")
print(f"Average F1@10: {average_f1_at_k}")
print(f"Average Results Quality: {average_quality}")
print(f"Average MAP@40: {average_avg_precision}")

average_retrieval_time = time.time() - t_start
print(f"Avergae Retreival Time for {len(queries.keys())} Queries = {average_retrieval_time}")