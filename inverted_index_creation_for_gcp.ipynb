{"cells":[{"cell_type":"markdown","metadata":{"id":"ByCSu-FhG5v7"},"source":["# Project Pre-Processing : Creating Inverted Indexes for anchor, text and title of wikipedia files"]},{"cell_type":"markdown","metadata":{"id":"-e3cxxyFG_XW"},"source":["## Overview\n","\n","The goal of this notebook is to process the necessary inverse index in such a way that we can eventually store it in our GCP bucket without having to read the index again, thus saving space and runtime.\n","The processing will be on any Wikipedia file that we have already saved in bucket, on which we will perform all kinds of operations in order to create a general structure of (token, (doc_id, tf)). This structure is the basis by which we can read the posting list into the bucket, save them in binary files and then read them back directly from the bucket.\n","In the end, each of them will have their own dictionary.\n","After that we will create a new folder that will basically contain the latest dictionary from all indexes, its bin files and the new dictionary that will be in the pkl file.\n","\n","We can later read the pkl file from the bucket and use it to perform retrieval operations, answer queries and calculate metrics.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J0P4W63JvrT5"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"FdJVyfm4EmV8"},"source":["## Cells relevant to cluster creation and working with GCP directly\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1Lf-tk7E31e","outputId":"3a58dc6a-b6e9-464e-86ed-77780a6f1bc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-4d56  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable\n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","metadata":{"id":"59yMYAN1x3oh"},"source":["## General imports\n","\n","The `inverted_index_gcp` import requires the `inverted_index_gcp.py` file. You should upload the file and then run this cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBMncIJ2E7Mf","outputId":"f6637767-374d-48e3-d8a4-6b671ee21cdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"byNgReCYsmA9","nbgrader":{"cell_type":"code","checksum":"9a5e34081c68da39370c09ff0ee046f0","grade":false,"grade_id":"cell-4bab9532110cc7e3","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"ea863040-8aaa-4840-9b5f-a0d13a06dbd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import sys\n","from collections import Counter, OrderedDict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from timeit import timeit\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from google.cloud import storage\n","import builtins\n","import math\n","\n","stemmer = PorterStemmer()\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KRjOVgLBrfCs"},"source":["## Installing, importing, and initializing PySpark\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"BkEErqJnsXyy","nbgrader":{"cell_type":"code","checksum":"032f9e425c679e50c3f5c4ea0c8c1a0d","grade":false,"grade_id":"cell-eb29d235a981ccf1","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from graphframes import *\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YIDnKER9eSP","outputId":"8d5dcd36-37ec-4a13-9943-c6019ba7895f"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-4d56-m.c.task3mapreduce315537936.internal:40581\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f9622f406d0>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","metadata":{"id":"-12thGyYFgGd"},"source":["\n","## Checking inverted_index_gcp is in our dataproc. If it's not there, upload it.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrdogGWJKb27","outputId":"aa9c589e-071a-43dd-a2b1-04e70cdf3d9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","metadata":{"id":"19EG5AvVru1J"},"source":["## Load the wiki dump files directly from the bucket into one big corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYNfYr4SF6dR","outputId":"d6579ab7-6f7d-43a3-a74b-2ba400ad81a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["bucket_name = '315537936'\n","full_path = f\"gs://{bucket_name}/\"\n","paths = []\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name.endswith('.parquet') and b.name != 'graphframes.sh':\n","        paths.append(full_path + b.name)\n","\n","corpus = spark.read.parquet(*paths)"]},{"cell_type":"markdown","metadata":{"id":"otkgtIMdkUuK"},"source":["## After GCP setup completed, we can start creating wanted Inverted Indexes"]},{"cell_type":"markdown","metadata":{"id":"7keCGifFmabV"},"source":["First of all, we will check the number of files in the corpus, just like we have seen in HW3, the count needs to be more than 6M"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTykBbWgGbdp","outputId":"09208d83-d5e5-494f-f7cc-a81fb09433a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Count number of wiki pages\n","corpus_size = corpus.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmuBLSnoy6ls","outputId":"401624f9-5e8c-4ab6-ad77-5a5a95063077"},"outputs":[{"name":"stdout","output_type":"stream","text":["6348910\n"]}],"source":["#CHECKING\n","print(corpus_size)"]},{"cell_type":"markdown","metadata":{"id":"rhbB-REwmiy7"},"source":["First functions will be related to count words and sorting them.\n","Unlike HW3, we modified this functions in order to be able to do phrasing  (in some cases, not in all cases phrasing is needed)- the main goal is to decrease the number of words to search later during query.\n","We will create three methods of stemming and we will apply them mainly on the body text index.\n","In title and anchor text, stemming is less common because there is a significance to the original text.For example, An article \"Studies\" and \"Stud\" is not the same article.   \n","\n","The methods are -\n","* stemming by porter's\n","\n","* N-gram (N=2,N=3)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"odXQ3K_q0Bx5"},"source":["Furthermore, we will use later CosSim function. We will calculate for each document the denominator for all relavant words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc0ADJ2umpHJ"},"outputs":[],"source":["CORPUS_SIZE = 6348910\n","import math\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","def word_count(id, text, stem=False, n2gram=False, n3gram=False):\n","    \"\"\"\n","    Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","    There is an option of stemming, 2-gram, 3-gram, or none stemming on the text body.\n","\n","    Parameters:\n","    -----------\n","    text: str\n","        Text of one document\n","    id: int\n","        Document id\n","    stem: boolean\n","        Option for stemming\n","    n2gram: boolean\n","        Option for 2-gram\n","    n3gram: boolean\n","        Option for 3-gram\n","\n","    Returns:\n","    --------\n","    List of tuples\n","        A list of (token, (doc_id, tf)) pairs,\n","        for example: [(\"Anarchism\", (12, 5)), ...]. Token can be stemmed or not depending on the boolean parameters\n","    \"\"\"\n","    # Tokenization and stemming\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n","\n","    # N-grams\n","    if n2gram or n3gram:\n","        if len(tokens) < 2:\n","            return []\n","\n","        if n2gram:\n","            bigram = list(ngrams(tokens, 2))\n","            tokens += [' '.join(b) for b in bigram]\n","\n","        if n3gram:\n","            trigram = list(ngrams(tokens, 3))\n","            tokens += [' '.join(t) for t in trigram]\n","\n","    # Counting frequencies\n","    counter = Counter(tokens)\n","    result = []\n","    # Filtering out stopwords and preparing result\n","    result = [(token, (id, tf)) for token, tf in counter.items() if token not in all_stopwords]\n","\n","    return result\n","\n","def cal_len_tokens(id, text, stem=False, n2gram=False, n3gram=False):\n","    \"\"\"\n","    Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","    There is an option of stemming, 2-gram, 3-gram, or none stemming on the text body.\n","    Afterwards, calculate the number of tokens\n","\n","    Parameters:\n","    -----------\n","    text: str\n","        Text of one document\n","    id: int\n","        Document id\n","    stem: boolean\n","        Option for stemming\n","    n2gram: boolean\n","        Option for 2-gram\n","    n3gram: boolean\n","        Option for 3-gram\n","\n","    Returns:\n","    --------\n","    List of tuples (doc_id, length) for each document\n","    \"\"\"\n","    # Tokenization and stemming\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n","\n","    # N-grams\n","    if n2gram or n3gram:\n","        if len(tokens) < 2:\n","            return []\n","\n","        if n2gram:\n","            bigram = list(ngrams(tokens, 2))\n","            tokens += [' '.join(b) for b in bigram]\n","\n","        if n3gram:\n","            trigram = list(ngrams(tokens, 3))\n","            tokens += [' '.join(t) for t in trigram]\n","\n","    # Counting frequencies\n","    counter = Counter(tokens)\n","    result = [(token, (id, tf)) for token, tf in counter.items() if token not in all_stopwords]\n","    result_tokens = [token for token, _ in result]\n","    results = [(id, len(result_tokens))]\n","\n","    return results\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","        unsorted_pl: list of tuples\n","            A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","        list of tuples\n","            A sorted posting list.\n","    '''\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def word_count_anchor(data_tuple):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","     In anchor - we don't want to so stemming.\n","    Parameters:\n","    -----------\n","      data_tuple: tuple\n","        A tuple containing (id, (doc_id, text))\n","    Returns:\n","    --------\n","      List of tuples\n","        A list of (doc_id, (token, tf)) pairs\n","        for example: [(12, ('Anarchism', 5)), ...]\n","    '''\n","    id, (doc_id, text) = data_tuple\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    counter = Counter(tokens)\n","    result = [(id, (token, tf)) for token, tf in counter.items() if token not in all_stopwords]\n","    return result\n","\n","\n","def calc_tfidf_normalized(doc_id, text, df_corpus, corpus_size, stem=False, n2gram=False, n3gram=False):\n","    # Calculate the word counts using the word_count function\n","    word_counts = word_count(doc_id, text, stem, n2gram, n3gram)\n","    # Extracting (token, tf) pairs from the result of word_count\n","    tokens = [token for token, (_, tf) in word_counts]\n","    words_counter = Counter(tokens)\n","    tokens_length = len(tokens)\n","\n","    # Calculate the normalized TD-IDF size - it will be normalized by the tokens_length\n","    size = builtins.sum([(count /tokens_length * math.log2(corpus_size/df_corpus[word]))**2 for word, count in words_counter.items() if word in df_corpus])\n","\n","\n","    normalized_tfidf_size = math.sqrt(size)\n","\n","\n","    return (doc_id, normalized_tfidf_size)"]},{"cell_type":"markdown","metadata":{"id":"fkZmqU4ym9E3"},"source":["Now it will be functions that help us to recieve wanted RDD startctue, using MapReduce"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u061hUc7nF5C"},"outputs":[],"source":["def calculate_term_total(posting):\n","    \"\"\"Calculate the term total for each posting from the corpus \"\"\"\n","    return posting.map(lambda x: (x[0],builtins.sum([i[1] for i in x[1]])))\n","\n","def calculate_df(posting):\n","\n","    \"\"\"\n","    calculate doc frequency of each posting term from the corpus\n","    \"\"\"\n","\n","    return posting.map(lambda x: (x[0], len(x[1])))\n","\n","def calculate_doc_length(corpus, column, stem, n2gram, n3gram):\n","    \"\"\"Calculate the document length for each one in the corpus\"\"\"\n","    doc_pairs = corpus.select(\"id\", column).rdd\n","    all_tokens_rdd = doc_pairs.flatMap(lambda x: cal_len_tokens(x[0], x[1], stem, n2gram, n3gram))\n","    token_length = all_tokens_rdd.collect()\n","    token_length_dict = dict(token_length)\n","\n","    return token_length_dict\n","\n","\n","def tf_idf_nz(corpus, column, w2df_dict,corpus_size, stem, n2gram, n3gram):\n","    \"\"\"Calculate the tf-idf normalized for each one in the corpus\"\"\"\n","    doc_pairs = corpus.select(\"id\", column).rdd\n","    tf_score_df = doc_pairs.map(lambda x: calc_tfidf_normalized(x[0], x[1], w2df_dict, corpus_size, stem, n2gram, n3gram))\n","    token_score = tf_score_df.collect()\n","    token_length_dict = dict(token_score)\n","\n","\n","    return token_length_dict\n","\n","\n","\n","def create_posting_list_for_text_or_title(corpus, column_name, filter_number=50, stem=False, n2gram=False, n3gram=False):\n","    # [(12, 'Anarchism'), (25, 'Autism'), (39, 'Albedo')....]\n","    doc_text_pairs = corpus.limit(600000).select(\"id\", column_name).rdd.map(lambda r: (r['id'], r[column_name]))\n","    #[('anarchism', (12, 1)), ('autism', (25, 1)), ('albedo', (39, 1))....]\n","    all_text_tuples_rdd = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1], stem, n2gram, n3gram))\n","    #[('autism', [(25, 1)]), ('abraham', [(307, 1), (1436, 1), (2851, 1)])...]\n","    posting_text = all_text_tuples_rdd.groupByKey().mapValues(reduce_word_counts)\n","    if column_name == \"text\":\n","        posting_text = posting_text.filter(lambda x: len(x[1]) > filter_number)\n","    return posting_text\n","\n","\n","def create_posting_list_for_anchor(corpus):\n","    #[(12, [Row(id=23040, text='political philosophy'), Row(id=99232, text='movement')...]\n","    doc_anchor_pairs = corpus.select(\"anchor_text\", \"id\").rdd.map(lambda r: (r['id'], r['anchor_text']))\n","    #[(12, (23040, 'political philosophy')), (12, (99232, 'movement'))...]\n","    transformed_data = doc_anchor_pairs.flatMap(lambda x: [(x[0], (row.id, row.text)) for row in x[1]])\n","    #[(12, ('political', 1)), (12, ('philosophy', 1)), (12, ('movement', 1)...]\n","    another_transformation = transformed_data.flatMap(word_count_anchor)\n","    #[(12, 'political'), (12, 'philosophy'), (12, 'movement'), (12, 'authority')..]\n","    tokens_rdd = another_transformation.map(lambda x: (x[0], x[1][0]))\n","    #[((12, 'social'), 3), ((12, 'economics'), 1), ((12, 'collectivism'), 2)...]\n","    result_rdd = tokens_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n","    #[(12, ('social', 3)), (12, ('economics', 1)), (12, ('collectivism', 2))...]\n","    posting_anchor_before = result_rdd.map(lambda x: (x[0][0], (x[0][1], x[1])))\n","    #[('social', (12, 3)), ('economics', (12, 1)), ('collectivism', (12, 2))...]\n","    transformed_anchor = posting_anchor_before.map(lambda x: (x[1][0], (x[0], x[1][1])))\n","    #[('teaching', [(1148, 1), (1930, 1), (1938, 1), (3464, 1), (3747, 1), (4157, 1), (4501, 1), (4868, 3)....]\n","    posting_anchor = transformed_anchor.groupByKey().mapValues(reduce_word_counts)\n","    return posting_anchor"]},{"cell_type":"markdown","metadata":{"id":"3Q54O1qfumFb"},"source":["\n","Functions related to how to write and read an InvertedIndex and how to read a tuple of (doc_id,tf) from a bin file which is in the bucket.\n","This functions may be useful if we would like to write the posting lists locally.\n","Afterwards we will read it directly from InvertedIndex instance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBYUo4RdlNio"},"outputs":[],"source":["from google.cloud import storage\n","from google.cloud.exceptions import NotFound, GoogleCloudError\n","import pickle\n","from io import BytesIO\n","from pathlib import Path\n","from itertools import groupby\n","\n","NUM_BUCKETS=124\n","\n","def token2bucket_id(token):\n","    return int(_hash(token), 16) % NUM_BUCKETS\n","\n","def extract_and_write_postings(posting_rdd,base_dir, bucket_name):\n","    # Create directories if they don't exist\n","    Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","    # Partition Posting Lists into Buckets\n","    posting_into_buckets = posting_rdd.map(lambda x: (token2bucket_id(x[0]), x))\n","    grouped_buckets = posting_into_buckets.groupByKey()\n","    # All tokens will be in the same bucket and not in seperate - bucket,list(token,location)\n","    flattened_buckets = grouped_buckets.map(lambda x: (x[0], list([(word, locations) for word, locations in x[1]])))\n","    # Write Posting Lists to Disk so it will be (token,location)\n","    posting_locations_for_bucket = flattened_buckets.map(lambda x: InvertedIndex.write_a_posting_list((x[0], list(x[1])), base_dir, bucket_name))\n","\n","    # Collect bucket IDs\n","    bucket_ids = posting_locations_for_bucket.collect()\n","\n","    # Merge them locally\n","    merged_posting_locs = {}\n","    for bucket_id in bucket_ids:\n","        data = InvertedIndex.read_index(base_dir, f'{bucket_id}_posting_locs', bucket_name)\n","        if data is not None:\n","            for key, value in data.items():\n","                if key in merged_posting_locs:\n","                    merged_posting_locs[key].extend(value)\n","                else:\n","                    merged_posting_locs[key] = value\n","\n","\n","    return merged_posting_locs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wqex28bHxikd"},"source":["In the end, when we create an InvertedIndex on GCP, we would like to store in it many dictionaries that are nececcery for useful retrieval.\n","Some of them -\n","\n","1) Posting Locs\n","\n","2) (token,df)\n","\n","3) (token.term_total)\n","\n","4) (token, normalized score for CosSim)\n","\n","**NOTICE - posting locations directly we will be able to read with instance of InvertedIndex**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqLhhSkEwOiY"},"outputs":[],"source":["def create_index_for_text_or_title(corpus, column_name,kind_of_index, stem=False, n2gram=False, n3gram=False):\n","    t_start = time()\n","\n","    #two_new_lists that will help us to retreival it faster during query\n","    final_posting_locs_rdd = create_posting_list_for_text_or_title(corpus,column_name,stem,n2gram,n3gram)\n","    final_posting_locs = extract_and_write_postings(final_posting_locs_rdd,f\"{kind_of_index}\",bucket_name)\n","    term_total = calculate_term_total(final_posting_locs_rdd).collectAsMap()\n","    term_frequency = calculate_df(final_posting_locs_rdd).collectAsMap()\n","    term_frequency_counter = Counter(term_frequency)\n","    documents_length = calculate_doc_length(corpus,column_name,stem,n2gram,n3gram)\n","    documents_normalized_length = tf_idf_nz(corpus, column_name, term_frequency_counter,CORPUS_SIZE, stem, n2gram, n3gram)\n","    index = InvertedIndex()\n","    index.posting_locs = final_posting_locs\n","    index.term_total = term_total\n","    index.df = term_frequency\n","    index.document_length = documents_length\n","    index.normalized_length = documents_normalized_length\n","    index.write_index(f\"{kind_of_index}\", f\"{kind_of_index}_index\",bucket_name)\n","\n","    # we need to upload it to our bucket\n","    index_const_time = time() - t_start\n","    print(f\"index_time = {index_const_time}\")\n","\n","\n","def create_index_for_anchor(corpus,kind_of_index):\n","    t_start = time()\n","\n","    #two_new_lists that will help us to retreival it faster during query\n","    final_posting_locs_rdd = create_posting_list_for_anchor(corpus)\n","    final_posting_locs = extract_and_write_postings(final_posting_locs_rdd,f\"{kind_of_index}\",bucket_name)\n","    term_total = calculate_term_total(final_posting_locs).collectAsMap()\n","    term_frequency = calculate_df(final_posting_locs).collectAsMap()\n","\n","    index = InvertedIndex()\n","    index.posting_locs = final_posting_locs\n","    index.term_total = term_total\n","    index.df = term_frequency\n","    index.write_index(f\"{kind_of_index}\", f\"{kind_of_index}_index\",bucket_name)\n","\n","    # we need to upload it to our bucket\n","    index_const_time = time() - t_start\n","    print(f\"index_time = {index_const_time}\")\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W1C1wt9YDVV7"},"source":["**Creating the indexes**\n","\n","1) Text Body without stemming\n","\n","2) Text Body with Porter's stemming\n","\n","3) Text Body with N-Gram\n","\n","4) Title\n","\n","5) Anchor\n"]},{"cell_type":"markdown","metadata":{"id":"bO1cQ6FbDl_Z"},"source":["**Title**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihnUfsaNDq70","outputId":"1e2d300c-aec1-4bf1-ee8d-0850d830401e","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["index_time = 136.547283411026\n"]}],"source":["title_index = create_index_for_text_or_title(corpus, \"title\",\"title\", stem=False, n2gram=False, n3gram=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["anchor_index = create_index_for_anchor(corpus, \"anchor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lj6c-6B5mH4u","outputId":"87527ab1-ac50-47d2-834a-fc3f357de689"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_15 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_48 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_100 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_74 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_15 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_58 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_74 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_92 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_31 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_106 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_15 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_0 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_23 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_91 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_66 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_32 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_48 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_114 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_66 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_114_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_32 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_23 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_0 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_48 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_0 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_100 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_110_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_58 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_106 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_91 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_106 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_15 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_74 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_41 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_91 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_58 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_66 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_23 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_82 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_41 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_23 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_58 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_15 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_106 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_82 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_74 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_91 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_23 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_31 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_66 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_114 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_74 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_82 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_100 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_0 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_41 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_66 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_48 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_48 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_114 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_31 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_122 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_31 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_82 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_0 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_32 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_92 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_58 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_32 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_82 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_106 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_100 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_31 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_100 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_32 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_91 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_114 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_114 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_41 !\n","24/03/05 23:35:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_41 !\n","24/03/05 23:35:56 WARN YarnAllocator: Container from a bad node: container_1709674750463_0002_01_000109 on host: cluster-2bfa-w-2.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:35:56.168]Container killed on request. Exit code is 143\n","[2024-03-05 23:35:56.168]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:35:56.169]Killed by external signal\n",".\n","24/03/05 23:35:56 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 109 for reason Container from a bad node: container_1709674750463_0002_01_000109 on host: cluster-2bfa-w-2.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:35:56.168]Container killed on request. Exit code is 143\n","[2024-03-05 23:35:56.168]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:35:56.169]Killed by external signal\n",".\n","24/03/05 23:35:56 ERROR YarnScheduler: Lost executor 109 on cluster-2bfa-w-2.c.task3mapreduce315537936.internal: Container from a bad node: container_1709674750463_0002_01_000109 on host: cluster-2bfa-w-2.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:35:56.168]Container killed on request. Exit code is 143\n","[2024-03-05 23:35:56.168]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:35:56.169]Killed by external signal\n",".\n","24/03/05 23:35:56 WARN TaskSetManager: Lost task 0.0 in stage 306.0 (TID 5631) (cluster-2bfa-w-2.c.task3mapreduce315537936.internal executor 109): ExecutorLostFailure (executor 109 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709674750463_0002_01_000109 on host: cluster-2bfa-w-2.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:35:56.168]Container killed on request. Exit code is 143\n","[2024-03-05 23:35:56.168]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:35:56.169]Killed by external signal\n",".\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_54 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_16 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_85 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_93 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_42 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_33 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_77 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_70 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_62 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_33 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_114_117 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_62 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_62 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_16 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_42 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_123 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_101 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_25 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_93 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_39 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_115 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_25 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_110_117 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_2 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_39 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_54 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_101 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_3 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_3 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_3 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_39 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_39 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_115 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_42 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_117 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_42 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_25 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_3 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_70 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_85 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_101 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_54 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_9 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_54 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_70 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_33 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_9 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_123 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_85 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_85 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_2 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_9 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_93 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_3 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_39 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_77 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_77 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_9 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_70 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_115 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_123 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_33 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_2 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_117 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_93 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_123 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_16 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_85 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_25 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_123 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_109 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_115 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_54 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_101 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_77 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_93 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_42 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_25 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_9 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_101 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_70 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_2 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_62 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_2 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_16 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_115 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_77 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_33 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_16 !\n","24/03/05 23:36:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_62 !\n","24/03/05 23:36:04 WARN YarnAllocator: Container from a bad node: container_1709674750463_0002_01_000110 on host: cluster-2bfa-w-3.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:36:04.798]Container killed on request. Exit code is 143\n","[2024-03-05 23:36:04.798]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:36:04.798]Killed by external signal\n",".\n","24/03/05 23:36:04 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 110 for reason Container from a bad node: container_1709674750463_0002_01_000110 on host: cluster-2bfa-w-3.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:36:04.798]Container killed on request. Exit code is 143\n","[2024-03-05 23:36:04.798]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:36:04.798]Killed by external signal\n",".\n","24/03/05 23:36:04 ERROR YarnScheduler: Lost executor 110 on cluster-2bfa-w-3.c.task3mapreduce315537936.internal: Container from a bad node: container_1709674750463_0002_01_000110 on host: cluster-2bfa-w-3.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:36:04.798]Container killed on request. Exit code is 143\n","[2024-03-05 23:36:04.798]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:36:04.798]Killed by external signal\n",".\n","24/03/05 23:36:04 WARN TaskSetManager: Lost task 0.1 in stage 306.0 (TID 5632) (cluster-2bfa-w-3.c.task3mapreduce315537936.internal executor 110): ExecutorLostFailure (executor 110 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709674750463_0002_01_000110 on host: cluster-2bfa-w-3.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:36:04.798]Container killed on request. Exit code is 143\n","[2024-03-05 23:36:04.798]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:36:04.798]Killed by external signal\n",".\n","ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","  File \"/opt/conda/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n","    return self._sock.recv_into(b)\n","KeyboardInterrupt\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_11711/1074081421.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbody_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_index_for_text_or_title\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"text_nostem\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstem\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn2gram\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn3gram\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/tmp/ipykernel_11711/829463326.py\u001B[0m in \u001B[0;36mcreate_index_for_text_or_title\u001B[0;34m(corpus, column_name, kind_of_index, stem, n2gram, n3gram)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;31m#two_new_lists that will help us to retreival it faster during query\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mfinal_posting_locs_rdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_posting_list_for_text_or_title\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mn2gram\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mn3gram\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mfinal_posting_locs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_and_write_postings\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34mf\"{kind_of_index}\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbucket_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mterm_total\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalculate_term_total\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mterm_frequency\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalculate_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/tmp/ipykernel_11711/2258564342.py\u001B[0m in \u001B[0;36mextract_and_write_postings\u001B[0;34m(posting_rdd, base_dir, bucket_name)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;31m# Collect bucket IDs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0mbucket_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mposting_locations_for_bucket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;31m# Merge them locally\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1318\u001B[0m             \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1320\u001B[0;31m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m         \u001B[0mconnection\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1037\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1038\u001B[0;31m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1039\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_connection_guard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    510\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 511\u001B[0;31m                 \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msmart_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    512\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Answer received: {0}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;31m# Happens when a the other end is dead. There might be an empty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/opt/conda/miniconda3/lib/python3.10/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 705\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    706\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    707\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"source":["body_index = create_index_for_text_or_title(corpus, \"text\",\"text_nostem\", stem=False, n2gram=False, n3gram=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnGusXOZmH4u","outputId":"d2fb729d-a64f-4a4a-fdaf-9567e8bc0be3"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_28 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_89 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_96 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_28 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_21 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_108 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_89 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_13 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_68 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_50 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_28 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_21 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_59 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_118 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_28 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_75 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_59 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_83 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_38 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_118 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_13 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_50 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_118 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_108 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_68 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_13 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_21 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_7 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_7 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_118 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_68 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_68 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_83 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_96 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_50 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_89 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_13 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_75 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_38 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_45 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_83 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_21 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_96 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_59 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_7 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_38 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_68 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_38 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_7 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_7 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_45 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_75 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_96 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_45 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_50 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_45 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_83 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_50 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_96 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_38 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_59 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_28 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_110 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_120 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_45 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_59 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_75 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_108 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_108 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_21 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_108 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_112_118 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_89 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_110_120 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_13 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_89 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_107_75 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_122_83 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_124_120 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_198_110 !\n","24/03/05 23:49:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_114_120 !\n","24/03/05 23:49:04 WARN YarnAllocator: Container from a bad node: container_1709674750463_0002_01_000111 on host: cluster-2bfa-w-1.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:49:03.297]Container killed on request. Exit code is 143\n","[2024-03-05 23:49:03.297]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:49:03.297]Killed by external signal\n",".\n","24/03/05 23:49:04 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 111 for reason Container from a bad node: container_1709674750463_0002_01_000111 on host: cluster-2bfa-w-1.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:49:03.297]Container killed on request. Exit code is 143\n","[2024-03-05 23:49:03.297]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:49:03.297]Killed by external signal\n",".\n","24/03/05 23:49:04 ERROR YarnScheduler: Lost executor 111 on cluster-2bfa-w-1.c.task3mapreduce315537936.internal: Container from a bad node: container_1709674750463_0002_01_000111 on host: cluster-2bfa-w-1.c.task3mapreduce315537936.internal. Exit status: 143. Diagnostics: [2024-03-05 23:49:03.297]Container killed on request. Exit code is 143\n","[2024-03-05 23:49:03.297]Container exited with a non-zero exit code 143. \n","[2024-03-05 23:49:03.297]Killed by external signal\n",".\n","ERROR:root:KeyboardInterrupt while sending command.][Stage 314:>  (0 + 1) / 1]1]\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","  File \"/opt/conda/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n","    return self._sock.recv_into(b)\n","KeyboardInterrupt\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_11711/1413127552.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbody_stem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_index_for_text_or_title\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"text_stem\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstem\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn2gram\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn3gram\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/tmp/ipykernel_11711/829463326.py\u001B[0m in \u001B[0;36mcreate_index_for_text_or_title\u001B[0;34m(corpus, column_name, kind_of_index, stem, n2gram, n3gram)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;31m#two_new_lists that will help us to retreival it faster during query\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mfinal_posting_locs_rdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_posting_list_for_text_or_title\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mn2gram\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mn3gram\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mfinal_posting_locs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_and_write_postings\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34mf\"{kind_of_index}\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbucket_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mterm_total\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalculate_term_total\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mterm_frequency\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalculate_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_posting_locs_rdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/tmp/ipykernel_11711/2258564342.py\u001B[0m in \u001B[0;36mextract_and_write_postings\u001B[0;34m(posting_rdd, base_dir, bucket_name)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;31m# Collect bucket IDs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0mbucket_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mposting_locations_for_bucket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;31m# Merge them locally\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1318\u001B[0m             \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1320\u001B[0;31m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m         \u001B[0mconnection\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1037\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1038\u001B[0;31m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1039\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_connection_guard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    510\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 511\u001B[0;31m                 \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msmart_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    512\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Answer received: {0}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;31m# Happens when a the other end is dead. There might be an empty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/opt/conda/miniconda3/lib/python3.10/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 705\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    706\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    707\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"source":["body_stem = create_index_for_text_or_title(corpus, \"text\",\"text_stem\", stem=True, n2gram=False, n3gram=False)"]},{"cell_type":"markdown","metadata":{"id":"bbVdKC5Nv4p_"},"source":["Now let's read the pkl file and see the dictionaries acceptable by O(1) retreival\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I6n1d8pBBURJ"},"source":["**Other function that help us to store another pkl files that we may use them later - {doc:id,title} dict, page rank dict, page views dict**"]},{"cell_type":"markdown","metadata":{"id":"qTxZ1FxBfNc-"},"source":["## Creating {doc_id:title} dict\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBICd-pBfWry","outputId":"18d0dbc2-1ec5-4313-ecf1-b8ada989e726"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_title_rdd = corpus.select(\"id\", \"title\").rdd\n","doc_title_dict = doc_title_rdd.map(lambda row: (row.id, row.title)).collectAsMap()\n","\n","# Writing to GCS using google-cloud-storage\n","client = storage.Client()\n","bucket = client.get_bucket(\"315537936\")\n","blob = bucket.blob(\"doc_title_dict.pkl\")\n","with blob.open(\"wb\") as pkl_file:\n","    pickle.dump(doc_title_dict, pkl_file)"]},{"cell_type":"markdown","metadata":{"id":"I2oCcQwP5zS_"},"source":["## PageRank"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"YFRogCnlHIGR","nbgrader":{"cell_type":"code","checksum":"69c22317d1ad51b321d3ef4f86a4cb4f","grade":false,"grade_id":"cell-d0d3cd0a8e67f3d5","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and\n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the\n","      second entry is the destination page id. No duplicates should be present.\n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph\n","      created by the wikipedia links. No duplicates should be present.\n","  '''\n","  # #{id=1,anchor_text={1,'tokens...'},{2,'tokens',....}}-> {(1,1),(1,2),(1,3)}\n","  pair_list = pages.flatMap(lambda x: [(x[0], entry[0]) for entry in x[1]])\n","\n","  # edges need to be unique by pairs\n","  #if (1,3) exists (3,1) don't\n","  edges = pair_list.distinct()\n","\n","  # vertices need to be unique by individual\n","  # final map in order that it will fit for dataframe\n","  vertices = pair_list.flatMap(lambda x: [x[0], x[1]]).distinct().map(lambda x: (x,))\n","\n","  return edges, vertices\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"ajIiUP8CCY51","nbgrader":{"cell_type":"code","checksum":"d6f4f7646555fa5aed797d8eae7af96c","grade":true,"grade_id":"cell-bc0fda19ef37c287","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"outputId":"028e0439-bfd2-4749-9489-aae42cc73019"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/06 10:02:21 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://wikidata_preprocessed/*.\n","java.io.IOException: Error accessing gs://wikidata_preprocessed/*\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1141) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1115) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$getFileStatus$9(GoogleHadoopFileSystemBase.java:1073) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics.trackDuration(GhfsStorageStatistics.java:102) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1062) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1881) ~[hadoop-client-api-3.3.6.jar:?]\n","\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n","\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n","\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n","\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n","\tat java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n","\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/*?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"location\" : \"Authorization\",\n","    \"locationType\" : \"header\",\n","    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n","    \"reason\" : \"accountDisabled\"\n","  } ],\n","  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n","\t... 26 more\n","24/03/06 10:02:21 WARN TaskSetManager: Lost task 0.0 in stage 34.0 (TID 586) (cluster-4d56-w-2.c.task3mapreduce315537936.internal executor 66): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n","\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n","\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n","\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n","\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"location\" : \"Authorization\",\n","    \"locationType\" : \"header\",\n","    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n","    \"reason\" : \"accountDisabled\"\n","  } ],\n","  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n","\t... 23 more\n","\n","24/03/06 10:02:21 WARN TaskSetManager: Lost task 1.0 in stage 34.0 (TID 587) (cluster-4d56-w-2.c.task3mapreduce315537936.internal executor 66): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream11_part2_preprocessed.parquet\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n","\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n","\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n","\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n","\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream11_part2_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"location\" : \"Authorization\",\n","    \"locationType\" : \"header\",\n","    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n","    \"reason\" : \"accountDisabled\"\n","  } ],\n","  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n","\t... 23 more\n","\n","24/03/06 10:02:21 ERROR TaskSetManager: Task 0 in stage 34.0 failed 4 times; aborting job\n","24/03/06 10:02:21 WARN TaskSetManager: Lost task 1.3 in stage 34.0 (TID 593) (cluster-4d56-w-2.c.task3mapreduce315537936.internal executor 66): TaskKilled (Stage cancelled)\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1129.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 592) (cluster-4d56-w-2.c.task3mapreduce315537936.internal executor 66): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_10217/3144135252.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpages_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gs://wikidata_preprocessed/*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"anchor_text\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0medges\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerate_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpages_links\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mv_cnt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me_cnt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvertices\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medges\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0medgesDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0medges\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'src'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'dst'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepartition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m124\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'src'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    362\u001B[0m         )\n\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpaths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m     def text(\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1129.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 592) (cluster-4d56-w-2.c.task3mapreduce315537936.internal executor 66): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n"]}],"source":["pages_links = corpus.select(\"id\", \"anchor_text\").rdd\n","edges, vertices = generate_graph(pages_links)\n","\n","v_cnt, e_cnt = vertices.count(), edges.count()\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=5)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0kqtTHdOc4H"},"outputs":[],"source":["page_rank_rdd = pr.rdd.map(lambda row: (row['id'], row['pagerank']))\n","pr_dict = page_rank_rdd.collectAsMap()\n","\n","\n","# Writing to GCS using google-cloud-storage\n","client = storage.Client()\n","bucket = client.get_bucket(\"315537936\")\n","blob = bucket.blob(\"pagerank.pkl\")\n","with blob.open(\"wb\") as pkl_file:\n","    pickle.dump(pr_dict, pkl_file)"]},{"cell_type":"markdown","metadata":{"id":"fpRtUzYzsWdf"},"source":["##Page Views"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHdqDoRye1Io","outputId":"a12c030a-4561-4c7a-c4ed-91a8349ad4f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-03-06 08:38:27--  https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2\n","Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n","Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2503235912 (2.3G) [application/octet-stream]\n","Saving to: â€˜pageviews-202108-user.bz2â€™\n","\n","pageviews-202108-us 100%[===================>]   2.33G  3.87MB/s    in 11m 5s  \n","\n","2024-03-06 08:49:32 (3.59 MB/s) - â€˜pageviews-202108-user.bz2â€™ saved [2503235912/2503235912]\n","\n"]}],"source":["pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path)\n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","\n","!wget -N $pv_path\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})\n","\n","with open(pv_clean, 'wb') as f:\n","    pickle.dump(wid2pv, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuMGcgp3e1Io"},"outputs":[],"source":["\n","# Writing to GCS using google-cloud-storage\n","client = storage.Client()\n","bucket = client.get_bucket(\"315537936\")\n","blob = bucket.blob(\"pageviews.pkl\")\n","with blob.open(\"wb\") as pkl_file:\n","    pickle.dump(wid2pv, pkl_file)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}