{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCSu-FhG5v7"
      },
      "source": [
        "# Project Pre-Processing : Creating Inverted Indexes for anchor, text and title of wikipedia files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e3cxxyFG_XW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The goal of this notebook is to process the necessary inverse index in such a way that we can eventually store it in our GCP bucket without having to read the index again, thus saving space and runtime.\n",
        "The processing will be on any Wikipedia file that we have already saved in bucket, on which we will perform all kinds of operations in order to create a general structure of (token, (doc_id, tf)). This structure is the basis by which we can read the posting list into the bucket, save them in binary files and then read them back directly from the bucket.\n",
        "In the end, each of them will have their own dictionary.\n",
        "After that we will create a new folder that will basically contain the latest dictionary from all indexes, its bin files and the new dictionary that will be in the pkl file.\n",
        "\n",
        "We can later read the pkl file from the bucket and use it to perform retrieval operations, answer queries and calculate metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0P4W63JvrT5"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdJVyfm4EmV8"
      },
      "source": [
        "## Cells relevant to cluster creation and working with GCP directly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1Lf-tk7E31e"
      },
      "outputs": [],
      "source": [
        "# if the following command generates an error, you probably didn't enable\n",
        "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
        "# under Manage Security â†’ Project Access when setting up the cluster\n",
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59yMYAN1x3oh"
      },
      "source": [
        "## General imports\n",
        "\n",
        "The `inverted_index_gcp` import requires the `inverted_index_gcp.py` file. You should upload the file and then run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBMncIJ2E7Mf"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "byNgReCYsmA9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a5e34081c68da39370c09ff0ee046f0",
          "grade": false,
          "grade_id": "cell-4bab9532110cc7e3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import builtins\n",
        "import math\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjOVgLBrfCs"
      },
      "source": [
        "## Installing, importing, and initializing PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BkEErqJnsXyy",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "032f9e425c679e50c3f5c4ea0c8c1a0d",
          "grade": false,
          "grade_id": "cell-eb29d235a981ccf1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YIDnKER9eSP",
        "outputId": "8d5dcd36-37ec-4a13-9943-c6019ba7895f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-2736-m.c.task3mapreduce315537936.internal:41767\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f50fcab06d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-12thGyYFgGd"
      },
      "source": [
        "\n",
        "## Checking inverted_index_gcp is in our dataproc. If it's not there, upload it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrdogGWJKb27"
      },
      "outputs": [],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py\n",
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19EG5AvVru1J"
      },
      "source": [
        "## Load the wiki dump files directly from the bucket into one big corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYNfYr4SF6dR",
        "outputId": "d6579ab7-6f7d-43a3-a74b-2ba400ad81a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "bucket_name = '315537936'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths = []\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if b.name.endswith('.parquet') and b.name != 'graphframes.sh':\n",
        "        paths.append(full_path + b.name)\n",
        "\n",
        "corpus = spark.read.parquet(*paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otkgtIMdkUuK"
      },
      "source": [
        "## After GCP setup completed, we can start creating wanted Inverted Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7keCGifFmabV"
      },
      "source": [
        "First of all, we will check the number of files in the corpus, just like we have seen in HW3, the count needs to be more than 6M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTykBbWgGbdp",
        "outputId": "09208d83-d5e5-494f-f7cc-a81fb09433a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Count number of wiki pages\n",
        "corpus_size = corpus.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmuBLSnoy6ls",
        "outputId": "401624f9-5e8c-4ab6-ad77-5a5a95063077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6348910\n"
          ]
        }
      ],
      "source": [
        "#CHECKING\n",
        "print(corpus_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhbB-REwmiy7"
      },
      "source": [
        "First functions will be related to count words and sorting them.\n",
        "Unlike HW3, we modified this functions in order to be able to do phrasing  (in some cases, not in all cases phrasing is needed)- the main goal is to decrease the number of words to search later during query.\n",
        "We will create three methods of stemming and we will apply them mainly on the body text index.\n",
        "In title and anchor text, stemming is less common because there is a significance to the original text.For example, An article \"Studies\" and \"Stud\" is not the same article.   \n",
        "\n",
        "The methods are -\n",
        "* stemming by porter's\n",
        "\n",
        "* N-gram (N=2,N=3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odXQ3K_q0Bx5"
      },
      "source": [
        "Furthermore, we will use later CosSim function. We will calculate for each document the denominator for all relavant words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc0ADJ2umpHJ"
      },
      "outputs": [],
      "source": [
        "CORPUS_SIZE = 6348910\n",
        "import math\n",
        "from nltk.util import ngrams\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def word_count(id, text, stem=False, n2gram=False, n3gram=False):\n",
        "    \"\"\"\n",
        "    Count the frequency of each word in `text` (tf) that is not included in\n",
        "    `all_stopwords` and return entries that will go into our posting lists.\n",
        "    There is an option of stemming, 2-gram, 3-gram, or none stemming on the text body.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: str\n",
        "        Text of one document\n",
        "    id: int\n",
        "        Document id\n",
        "    stem: boolean\n",
        "        Option for stemming\n",
        "    n2gram: boolean\n",
        "        Option for 2-gram\n",
        "    n3gram: boolean\n",
        "        Option for 3-gram\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    List of tuples\n",
        "        A list of (token, (doc_id, tf)) pairs,\n",
        "        for example: [(\"Anarchism\", (12, 5)), ...]. Token can be stemmed or not depending on the boolean parameters\n",
        "    \"\"\"\n",
        "    # Tokenization and stemming\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n",
        "\n",
        "    # N-grams\n",
        "    if n2gram or n3gram:\n",
        "        if len(tokens) < 2:\n",
        "            return []\n",
        "\n",
        "        if n2gram:\n",
        "            bigram = list(ngrams(tokens, 2))\n",
        "            tokens += [' '.join(b) for b in bigram]\n",
        "\n",
        "        if n3gram:\n",
        "            trigram = list(ngrams(tokens, 3))\n",
        "            tokens += [' '.join(t) for t in trigram]\n",
        "\n",
        "    # Counting frequencies\n",
        "    counter = Counter(tokens)\n",
        "    result = []\n",
        "    # Filtering out stopwords and preparing result\n",
        "    result = [(token, (id, tf)) for token, tf in counter.items() if token not in all_stopwords]\n",
        "\n",
        "    return result\n",
        "\n",
        "def cal_len_tokens(id, text, stem=False, n2gram=False, n3gram=False):\n",
        "    \"\"\"\n",
        "    Count the frequency of each word in `text` (tf) that is not included in\n",
        "    `all_stopwords` and return entries that will go into our posting lists.\n",
        "    There is an option of stemming, 2-gram, 3-gram, or none stemming on the text body.\n",
        "    Afterwards, calculate the number of tokens\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: str\n",
        "        Text of one document\n",
        "    id: int\n",
        "        Document id\n",
        "    stem: boolean\n",
        "        Option for stemming\n",
        "    n2gram: boolean\n",
        "        Option for 2-gram\n",
        "    n3gram: boolean\n",
        "        Option for 3-gram\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    List of tuples (doc_id, length) for each document\n",
        "    \"\"\"\n",
        "    # Tokenization and stemming\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n",
        "\n",
        "    # N-grams\n",
        "    if n2gram or n3gram:\n",
        "        if len(tokens) < 2:\n",
        "            return []\n",
        "\n",
        "        if n2gram:\n",
        "            bigram = list(ngrams(tokens, 2))\n",
        "            tokens += [' '.join(b) for b in bigram]\n",
        "\n",
        "        if n3gram:\n",
        "            trigram = list(ngrams(tokens, 3))\n",
        "            tokens += [' '.join(t) for t in trigram]\n",
        "\n",
        "    # Counting frequencies\n",
        "    counter = Counter(tokens)\n",
        "    result = [(token, (id, tf)) for token, tf in counter.items() if token not in all_stopwords]\n",
        "    result_tokens = [token for token, _ in result]\n",
        "    results = [(id, len(result_tokens))]\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "    ''' Returns a sorted posting list by wiki_id.\n",
        "    Parameters:\n",
        "    -----------\n",
        "        unsorted_pl: list of tuples\n",
        "            A list of (wiki_id, tf) tuples\n",
        "    Returns:\n",
        "    --------\n",
        "        list of tuples\n",
        "            A sorted posting list.\n",
        "    '''\n",
        "    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n",
        "    return sorted_pl\n",
        "\n",
        "def word_count_anchor(data_tuple):\n",
        "    ''' Count the frequency of each word in `text` (tf) that is not included in\n",
        "    `all_stopwords` and return entries that will go into our posting lists.\n",
        "     In anchor - we don't want to so stemming.\n",
        "    Parameters:\n",
        "    -----------\n",
        "      data_tuple: tuple\n",
        "        A tuple containing (id, (doc_id, text))\n",
        "    Returns:\n",
        "    --------\n",
        "      List of tuples\n",
        "        A list of (doc_id, (token, tf)) pairs\n",
        "        for example: [(12, ('Anarchism', 5)), ...]\n",
        "    '''\n",
        "    id, (doc_id, text) = data_tuple\n",
        "\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    counter = Counter(tokens)\n",
        "    result = [(id, (token, tf)) for token, tf in counter.items() if token not in all_stopwords]\n",
        "    return result\n",
        "\n",
        "\n",
        "def calc_tfidf_normalized(doc_id, text, df_corpus, corpus_size, stem=False, n2gram=False, n3gram=False):\n",
        "    # Calculate the word counts using the word_count function\n",
        "    word_counts = word_count(doc_id, text, stem, n2gram, n3gram)\n",
        "    # Extracting (token, tf) pairs from the result of word_count\n",
        "    tokens = [token for token, (_, tf) in word_counts]\n",
        "    words_counter = Counter(tokens)\n",
        "    tokens_length = len(tokens)\n",
        "\n",
        "    # Calculate the normalized TD-IDF size - it will be normalized by the tokens_length\n",
        "    size = builtins.sum([(count /tokens_length * math.log2(corpus_size/df_corpus[word]))**2 for word, count in words_counter.items() if word in df_corpus])\n",
        "\n",
        "\n",
        "    normalized_tfidf_size = math.sqrt(size)\n",
        "\n",
        "\n",
        "    return (doc_id, normalized_tfidf_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkZmqU4ym9E3"
      },
      "source": [
        "Now it will be functions that help us to recieve wanted RDD startctue, using MapReduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u061hUc7nF5C"
      },
      "outputs": [],
      "source": [
        "def calculate_term_total(posting):\n",
        "    \"\"\"Calculate the term total for each posting from the corpus \"\"\"\n",
        "    return posting.map(lambda x: (x[0],builtins.sum([i[1] for i in x[1]])))\n",
        "\n",
        "def calculate_df(posting):\n",
        "\n",
        "    \"\"\"\n",
        "    calculate doc frequency of each posting term from the corpus\n",
        "    \"\"\"\n",
        "\n",
        "    return posting.map(lambda x: (x[0], len(x[1])))\n",
        "\n",
        "def calculate_doc_length(corpus, column, stem, n2gram, n3gram):\n",
        "    \"\"\"Calculate the document length for each one in the corpus\"\"\"\n",
        "    doc_pairs = corpus.select(\"id\", column).rdd\n",
        "    all_tokens_rdd = doc_pairs.flatMap(lambda x: cal_len_tokens(x[0], x[1], stem, n2gram, n3gram))\n",
        "    token_length = all_tokens_rdd.collect()\n",
        "    token_length_dict = dict(token_length)\n",
        "\n",
        "    return token_length_dict\n",
        "\n",
        "\n",
        "def tf_idf_nz(corpus, column, w2df_dict,corpus_size, stem, n2gram, n3gram):\n",
        "    \"\"\"Calculate the tf-idf normalized for each one in the corpus\"\"\"\n",
        "    doc_pairs = corpus.select(\"id\", column).rdd\n",
        "    tf_score_df = doc_pairs.map(lambda x: calc_tfidf_normalized(x[0], x[1], w2df_dict, corpus_size, stem, n2gram, n3gram))\n",
        "    token_score = tf_score_df.collect()\n",
        "    token_length_dict = dict(token_score)\n",
        "\n",
        "\n",
        "    return token_length_dict\n",
        "\n",
        "\n",
        "\n",
        "def create_posting_list_for_text_or_title(corpus, column_name, filter_number=50, stem=False, n2gram=False, n3gram=False):\n",
        "    # [(12, 'Anarchism'), (25, 'Autism'), (39, 'Albedo')....]\n",
        "    doc_text_pairs = corpus.limit(600000).select(\"id\", column_name).rdd.map(lambda r: (r['id'], r[column_name]))\n",
        "    #[('anarchism', (12, 1)), ('autism', (25, 1)), ('albedo', (39, 1))....]\n",
        "    all_text_tuples_rdd = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1], stem, n2gram, n3gram))\n",
        "    #[('autism', [(25, 1)]), ('abraham', [(307, 1), (1436, 1), (2851, 1)])...]\n",
        "    posting_text = all_text_tuples_rdd.groupByKey().mapValues(reduce_word_counts)\n",
        "    if column_name == \"text\":\n",
        "        posting_text = posting_text.filter(lambda x: len(x[1]) > filter_number)\n",
        "    return posting_text\n",
        "\n",
        "\n",
        "def create_posting_list_for_anchor(corpus):\n",
        "    #[(12, [Row(id=23040, text='political philosophy'), Row(id=99232, text='movement')...]\n",
        "    doc_anchor_pairs = corpus.select(\"anchor_text\", \"id\").rdd.map(lambda r: (r['id'], r['anchor_text']))\n",
        "    #[(12, (23040, 'political philosophy')), (12, (99232, 'movement'))...]\n",
        "    transformed_data = doc_anchor_pairs.flatMap(lambda x: [(x[0], (row.id, row.text)) for row in x[1]])\n",
        "    #[(12, ('political', 1)), (12, ('philosophy', 1)), (12, ('movement', 1)...]\n",
        "    another_transformation = transformed_data.flatMap(word_count_anchor)\n",
        "    #[(12, 'political'), (12, 'philosophy'), (12, 'movement'), (12, 'authority')..]\n",
        "    tokens_rdd = another_transformation.map(lambda x: (x[0], x[1][0]))\n",
        "    #[((12, 'social'), 3), ((12, 'economics'), 1), ((12, 'collectivism'), 2)...]\n",
        "    result_rdd = tokens_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "    #[(12, ('social', 3)), (12, ('economics', 1)), (12, ('collectivism', 2))...]\n",
        "    posting_anchor_before = result_rdd.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
        "    #[('social', (12, 3)), ('economics', (12, 1)), ('collectivism', (12, 2))...]\n",
        "    transformed_anchor = posting_anchor_before.map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
        "    #[('teaching', [(1148, 1), (1930, 1), (1938, 1), (3464, 1), (3747, 1), (4157, 1), (4501, 1), (4868, 3)....]\n",
        "    posting_anchor = transformed_anchor.groupByKey().mapValues(reduce_word_counts)\n",
        "    return posting_anchor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q54O1qfumFb"
      },
      "source": [
        "\n",
        "Functions related to how to write and read an InvertedIndex and how to read a tuple of (doc_id,tf) from a bin file which is in the bucket.\n",
        "This functions may be useful if we would like to write the posting lists locally.\n",
        "Afterwards we will read it directly from InvertedIndex instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBYUo4RdlNio"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud.exceptions import NotFound, GoogleCloudError\n",
        "import pickle\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from itertools import groupby\n",
        "\n",
        "NUM_BUCKETS=124\n",
        "\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token), 16) % NUM_BUCKETS\n",
        "\n",
        "def extract_and_write_postings(posting_rdd,base_dir, bucket_name):\n",
        "    # Create directories if they don't exist\n",
        "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Partition Posting Lists into Buckets\n",
        "    posting_into_buckets = posting_rdd.map(lambda x: (token2bucket_id(x[0]), x))\n",
        "    grouped_buckets = posting_into_buckets.groupByKey()\n",
        "    # All tokens will be in the same bucket and not in seperate - bucket,list(token,location)\n",
        "    flattened_buckets = grouped_buckets.map(lambda x: (x[0], list([(word, locations) for word, locations in x[1]])))\n",
        "    # Write Posting Lists to Disk so it will be (token,location)\n",
        "    posting_locations_for_bucket = flattened_buckets.map(lambda x: InvertedIndex.write_a_posting_list((x[0], list(x[1])), base_dir, bucket_name))\n",
        "\n",
        "    # Collect bucket IDs\n",
        "    bucket_ids = posting_locations_for_bucket.collect()\n",
        "\n",
        "    # Merge them locally\n",
        "    merged_posting_locs = {}\n",
        "    for bucket_id in bucket_ids:\n",
        "        data = InvertedIndex.read_index(base_dir, f'{bucket_id}_posting_locs', bucket_name)\n",
        "        if data is not None:\n",
        "            for key, value in data.items():\n",
        "                if key in merged_posting_locs:\n",
        "                    merged_posting_locs[key].extend(value)\n",
        "                else:\n",
        "                    merged_posting_locs[key] = value\n",
        "\n",
        "\n",
        "    return merged_posting_locs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqex28bHxikd"
      },
      "source": [
        "In the end, when we create an InvertedIndex on GCP, we would like to store in it many dictionaries that are nececcery for useful retrieval.\n",
        "Some of them -\n",
        "\n",
        "1) Posting Locs\n",
        "\n",
        "2) (token,df)\n",
        "\n",
        "3) (token.term_total)\n",
        "\n",
        "4) (token, normalized score for CosSim)\n",
        "\n",
        "**NOTICE - posting locations directly we will be able to read with instance of InvertedIndex**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqLhhSkEwOiY"
      },
      "outputs": [],
      "source": [
        "def create_index_for_text_or_title(corpus, column_name,kind_of_index, stem=False, n2gram=False, n3gram=False):\n",
        "    t_start = time()\n",
        "\n",
        "    #two_new_lists that will help us to retreival it faster during query\n",
        "    final_posting_locs_rdd = create_posting_list_for_text_or_title(corpus,column_name,stem,n2gram,n3gram)\n",
        "    final_posting_locs = extract_and_write_postings(final_posting_locs_rdd,f\"{kind_of_index}\",bucket_name)\n",
        "    term_total = calculate_term_total(final_posting_locs_rdd).collectAsMap()\n",
        "    term_frequency = calculate_df(final_posting_locs_rdd).collectAsMap()\n",
        "    term_frequency_counter = Counter(term_frequency)\n",
        "    documents_length = calculate_doc_length(corpus,column_name,stem,n2gram,n3gram)\n",
        "    documents_normalized_length = tf_idf_nz(corpus, column_name, term_frequency_counter,CORPUS_SIZE, stem, n2gram, n3gram)\n",
        "    index = InvertedIndex()\n",
        "    index.posting_locs = final_posting_locs\n",
        "    index.term_total = term_total\n",
        "    index.df = term_frequency\n",
        "    index.document_length = documents_length\n",
        "    index.normalized_length = documents_normalized_length\n",
        "    index.write_index(f\"{kind_of_index}\", f\"{kind_of_index}_index\",bucket_name)\n",
        "\n",
        "    # we need to upload it to our bucket\n",
        "    index_const_time = time() - t_start\n",
        "    print(f\"index_time = {index_const_time}\")\n",
        "\n",
        "\n",
        "def create_index_for_anchor(corpus,kind_of_index):\n",
        "    t_start = time()\n",
        "\n",
        "    #two_new_lists that will help us to retreival it faster during query\n",
        "    final_posting_locs_rdd = create_posting_list_for_anchor(corpus)\n",
        "    final_posting_locs = extract_and_write_postings(final_posting_locs_rdd,f\"{kind_of_index}\",bucket_name)\n",
        "    term_total = calculate_term_total(final_posting_locs).collectAsMap()\n",
        "    term_frequency = calculate_df(final_posting_locs).collectAsMap()\n",
        "\n",
        "    index = InvertedIndex()\n",
        "    index.posting_locs = final_posting_locs\n",
        "    index.term_total = term_total\n",
        "    index.df = term_frequency\n",
        "    index.write_index(f\"{kind_of_index}\", f\"{kind_of_index}_index\",bucket_name)\n",
        "\n",
        "    # we need to upload it to our bucket\n",
        "    index_const_time = time() - t_start\n",
        "    print(f\"index_time = {index_const_time}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1C1wt9YDVV7"
      },
      "source": [
        "**Creating the indexes**\n",
        "\n",
        "1) Text Body without stemming, with stemming, with n-gram and with n-gram\n",
        "   and stemming\n",
        "\n",
        "2) Title Body without stemming, with stemming, with n-gram and with n-gram\n",
        "   and stemming\n",
        "\n",
        "3) Anchor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO1cQ6FbDl_Z"
      },
      "source": [
        "**Title**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihnUfsaNDq70",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "title_index = create_index_for_text_or_title(corpus, \"title\",\"title\", stem=False, n2gram=False, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbZh6q0yODJw"
      },
      "outputs": [],
      "source": [
        "title2gram_index = create_index_for_text_or_title(corpus, \"title\",\"title2gram\", stem=False, n2gram=True, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV9RU_CgODJw"
      },
      "outputs": [],
      "source": [
        "title_stemmed = create_index_for_text_or_title(corpus, \"title\",\"title_stem\", stem=True, n2gram=True, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wyr9IDmODJw"
      },
      "outputs": [],
      "source": [
        "title_stemmed_only = create_index_for_text_or_title(corpus, \"title\",\"title_stem_only\", stem=True, n2gram=False, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh-A4L2zODJw"
      },
      "outputs": [],
      "source": [
        "anchor_index = create_index_for_anchor(corpus, \"anchor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj6c-6B5mH4u"
      },
      "outputs": [],
      "source": [
        "body_index = create_index_for_text_or_title(corpus, \"text\",\"text\", stem=False, n2gram=False, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnGusXOZmH4u"
      },
      "outputs": [],
      "source": [
        "body_stem = create_index_for_text_or_title(corpus, \"text\",\"text_stem\", stem=True, n2gram=False, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NBMoR_5ODJx"
      },
      "outputs": [],
      "source": [
        "body_2gram = create_index_for_text_or_title(corpus, \"text\",\"text_2gram\", stem=False, n2gram=True, n3gram=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_yfOIPDODJx"
      },
      "outputs": [],
      "source": [
        "body_stem2gram = create_index_for_text_or_title(corpus, \"text\",\"text_stem2gram\", stem=True, n2gram=True, n3gram=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbVdKC5Nv4p_"
      },
      "source": [
        "Now let's read the pkl file and see the dictionaries acceptable by O(1) retreival\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6n1d8pBBURJ"
      },
      "source": [
        "**Other function that help us to store another pkl files that we may use them later - {doc:id,title} dict, page rank dict, page views dict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTxZ1FxBfNc-"
      },
      "source": [
        "## Creating {doc_id:title} dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBICd-pBfWry",
        "outputId": "18d0dbc2-1ec5-4313-ecf1-b8ada989e726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "doc_title_rdd = corpus.select(\"id\", \"title\").rdd\n",
        "doc_title_dict = doc_title_rdd.map(lambda row: (row.id, row.title)).collectAsMap()\n",
        "\n",
        "# Writing to GCS using google-cloud-storage\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(\"315537936\")\n",
        "blob = bucket.blob(\"doc_title_dict.pkl\")\n",
        "with blob.open(\"wb\") as pkl_file:\n",
        "    pickle.dump(doc_title_dict, pkl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2oCcQwP5zS_"
      },
      "source": [
        "## PageRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "YFRogCnlHIGR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "69c22317d1ad51b321d3ef4f86a4cb4f",
          "grade": false,
          "grade_id": "cell-d0d3cd0a8e67f3d5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_graph(pages):\n",
        "  ''' Compute the directed graph generated by wiki links.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    pages: RDD\n",
        "      An RDD where each row consists of one wikipedia articles with 'id' and\n",
        "      'anchor_text'.\n",
        "  Returns:\n",
        "  --------\n",
        "    edges: RDD\n",
        "      An RDD where each row represents an edge in the directed graph created by\n",
        "      the wikipedia links. The first entry should the source page id and the\n",
        "      second entry is the destination page id. No duplicates should be present.\n",
        "    vertices: RDD\n",
        "      An RDD where each row represents a vetrix (node) in the directed graph\n",
        "      created by the wikipedia links. No duplicates should be present.\n",
        "  '''\n",
        "  # #{id=1,anchor_text={1,'tokens...'},{2,'tokens',....}}-> {(1,1),(1,2),(1,3)}\n",
        "  pair_list = pages.flatMap(lambda x: [(x[0], entry[0]) for entry in x[1]])\n",
        "\n",
        "  # edges need to be unique by pairs\n",
        "  #if (1,3) exists (3,1) don't\n",
        "  edges = pair_list.distinct()\n",
        "\n",
        "  # vertices need to be unique by individual\n",
        "  # final map in order that it will fit for dataframe\n",
        "  vertices = pair_list.flatMap(lambda x: [x[0], x[1]]).distinct().map(lambda x: (x,))\n",
        "\n",
        "  return edges, vertices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ajIiUP8CCY51",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d6f4f7646555fa5aed797d8eae7af96c",
          "grade": true,
          "grade_id": "cell-bc0fda19ef37c287",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "pages_links = corpus.select(\"id\", \"anchor_text\").rdd\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "\n",
        "v_cnt, e_cnt = vertices.count(), edges.count()\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=5)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0kqtTHdOc4H"
      },
      "outputs": [],
      "source": [
        "page_rank_rdd = pr.rdd.map(lambda row: (row['id'], row['pagerank']))\n",
        "pr_dict = page_rank_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "# Writing to GCS using google-cloud-storage\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(\"315537936\")\n",
        "blob = bucket.blob(\"pagerank.pkl\")\n",
        "with blob.open(\"wb\") as pkl_file:\n",
        "    pickle.dump(pr_dict, pkl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpRtUzYzsWdf"
      },
      "source": [
        "##Page Views"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHdqDoRye1Io"
      },
      "outputs": [],
      "source": [
        "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
        "p = Path(pv_path)\n",
        "pv_name = p.name\n",
        "pv_temp = f'{p.stem}-4dedup.txt'\n",
        "pv_clean = f'{p.stem}.pkl'\n",
        "\n",
        "!wget -N $pv_path\n",
        "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        "\n",
        "wid2pv = Counter()\n",
        "with open(pv_temp, 'rt') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' ')\n",
        "        wid2pv.update({int(parts[0]): int(parts[1])})\n",
        "\n",
        "with open(pv_clean, 'wb') as f:\n",
        "    pickle.dump(wid2pv, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuMGcgp3e1Io"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Writing to GCS using google-cloud-storage\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(\"315537936\")\n",
        "blob = bucket.blob(\"pageviews.pkl\")\n",
        "with blob.open(\"wb\") as pkl_file:\n",
        "    pickle.dump(wid2pv, pkl_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}